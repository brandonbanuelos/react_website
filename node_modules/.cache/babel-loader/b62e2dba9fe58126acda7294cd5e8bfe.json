{"ast":null,"code":"import React,{useRef}from'react';import{ProjectsList}from'../helpers/ProjectsList';import'../styles/Projects.css';import ProjectItem from'../components/ProjectItem';import PySynthImage from\"../assets/pysynth.png\";import InstaBotImage from\"../assets/instabot.svg\";import I3DDemoImage from\"../assets/i3d.png\";import PatrolBotV1Image from\"../assets/patrolbotv1.png\";import PatrolBotV2Image from\"../assets/patrolbotv2.png\";import PatrolBotV3Image from\"../assets/patrolbotv3.png\";import{jsx as _jsx}from\"react/jsx-runtime\";import{jsxs as _jsxs}from\"react/jsx-runtime\";function Projects(){var PySynth=useRef(null);var InstaBot=useRef(null);var PatrolBotV1=useRef(null);var PatrolBotV2=useRef(null);var I3DDemo=useRef(null);var PatrolBotV3=useRef(null);var refArray=[PySynth,InstaBot,PatrolBotV1,PatrolBotV2,I3DDemo,PatrolBotV3];var scroll=function scroll(ref){if(ref.current){ref.current.scrollIntoView({behavior:\"smooth\"});}};return/*#__PURE__*/_jsxs(\"div\",{className:\"projects\",children:[/*#__PURE__*/_jsx(\"h1\",{className:\"projectsTitle\",children:\"My Projects\"}),/*#__PURE__*/_jsx(\"div\",{className:\"projectsList\",children:ProjectsList.map(function(projectItem,key){return/*#__PURE__*/_jsx(\"button\",{onClick:function onClick(){return scroll(refArray[key]);},children:/*#__PURE__*/_jsx(ProjectItem,{image:projectItem.image,name:projectItem.name,year:projectItem.year},key)});})}),/*#__PURE__*/_jsxs(\"div\",{className:\"projectsText\",children:[/*#__PURE__*/_jsx(\"h1\",{ref:PySynth,className:\"PySynth\",children:\"PySynth\"}),/*#__PURE__*/_jsx(\"img\",{src:PySynthImage}),/*#__PURE__*/_jsx(\"p\",{children:\"This project was term project for my first semester at university in 2019. It acted as a virtual analog synthesizer. I programmed this in Python using PyAudio, Tkinter, NumPy, and MatPlotLib. This project uses a crude form of UI, but it was my first real project using code. It was based on additive synthesis and had two oscillators that generated different types of sound waves based on NumPy functions. Click below to find my source code and my video presentation of it!\"}),/*#__PURE__*/_jsxs(\"div\",{className:\"buttonContainer\",children:[/*#__PURE__*/_jsx(\"button\",{type:\"button\",onClick:function onClick(e){e.preventDefault();window.location.href=\"https://youtu.be/OitXpHwO8Ho\";},className:\"PySynthVideo\",children:\"View Video\"}),/*#__PURE__*/_jsx(\"button\",{onClick:function onClick(e){e.preventDefault();window.location.href=\"https://github.com/brandonbanuelos/PySynth\";},className:\"PySynthCode\",children:\"View Code\"})]}),/*#__PURE__*/_jsx(\"h1\",{ref:InstaBot,className:\"InstaBot\",children:\"InstaBot\"}),/*#__PURE__*/_jsx(\"img\",{src:InstaBotImage}),/*#__PURE__*/_jsx(\"p\",{children:\"This project was something I did using Python and Selenium. It uses Selenium to access the HTML elements inside of Instagram's website, so it automates the browsing experience. Instagram is full of bots these days, so I wondered how difficult it would be for me to make one. This specific one logs in, goes to a specific hashtag, and likes and/or comments on posts depending on your user settings.\"}),/*#__PURE__*/_jsx(\"button\",{onClick:function onClick(e){e.preventDefault();window.location.href=\"https://github.com/brandonbanuelos/InstaBot\";},className:\"InstaBotCode\",children:\"View Code\"}),/*#__PURE__*/_jsx(\"h1\",{ref:PatrolBotV1,className:\"PatrolBotV1\",children:\"PatrolBotV1\"}),/*#__PURE__*/_jsx(\"img\",{src:PatrolBotV1Image}),/*#__PURE__*/_jsx(\"p\",{children:\"This project marked the beginning of my senior project with a team of four other students. This is an ambitious attempt to combine, software, robotics, and machine learning, to detect bike theft. This version was dubbed V1 after the fact because it was a prototype. It used primitive graphics and ran locally as a desktop software with only one machine learning model. It was programmed in Python using PyTorch, PyQt5, and NumPy. It was meant to be ran locally on a Raspberry Pi which controlled the robot, and the Pi would be VNC'd in order to view the feed. The machine learning model was trained on images of bikes, humans, bolt cutters, and angle grinders. If objects were detected, they would be placed into the logs, so the user could view any potentially dangerous threats. The repository for this one has been transformed into the the 1.5 version, so I will include the link in the next section.\"}),/*#__PURE__*/_jsx(\"h1\",{ref:PatrolBotV2,className:\"PatrolBotV2\",children:\"PatrolBotV2\"}),/*#__PURE__*/_jsx(\"img\",{src:PatrolBotV2Image}),/*#__PURE__*/_jsx(\"p\",{children:\"This version of the project marked the next step into the final product. This version was still ran locally, but it added a state-of-the-art Slow Fast Model trained on Kinetics 400 to detect actions through live video. This meant the robot could now detect aggressive actions coupled with the use of object detection to get a fuller picture of possible bike theft. This also added the use of IOU (intersection over union) of objects such as bolt cutters or angle grinders on top of bikes in order to provide a type of threat model algorithm Additionally, this version marked an increase in the UI components of the software. This time the UI was customized using CSS. The link below contains the 1.5 version of PatrolBot. This means it has the action detection model and algorithm, but it doesn't have the upgraded UI. This is because version 2 was still part of the prototype development.\"}),/*#__PURE__*/_jsx(\"button\",{onClick:function onClick(e){e.preventDefault();window.location.href=\"https://github.com/banuelos-brandon/PatrolBot\";},className:\"PatrolBotV1.5\",children:\"View Code\"}),/*#__PURE__*/_jsx(\"h1\",{ref:I3DDemo,className:\"I3DDemo\",children:\"I3D Demo\"}),/*#__PURE__*/_jsx(\"img\",{src:I3DDemoImage}),/*#__PURE__*/_jsx(\"p\",{children:\"This demo was created to show of my novel solution to a problem my group encountered during the development of PatrolBotV3. The issue was that the action detection model ran on mp4 files rather than lists or arrays of frames. PatrolBotV3 was the transformation from a locally-ran application to a web application, so saving and loading mp4 files would have added too much complication. Instead I decided to find a way to save a series of frames from the webcam and format them into a way that was readable by the Slow Fast or Inflated 3D models for action detection. This demo also shows off the training on this model done by my team. We trained this model to detect if behavior is aggressive or not. This demo takes webcam input and stores it during the duration of running time in the code, and predicts whether or not behavior from the video clip is agressive or not.\"}),/*#__PURE__*/_jsx(\"button\",{onClick:function onClick(e){e.preventDefault();window.location.href=\"https://github.com/brandonbanuelos/I3DDemo\";},className:\"I3DCode\",children:\"View Code\"}),/*#__PURE__*/_jsx(\"h1\",{ref:PatrolBotV3,className:\"PatrolBotV3\",children:\"PatrolBotV3\"}),/*#__PURE__*/_jsx(\"img\",{src:PatrolBotV3Image}),/*#__PURE__*/_jsx(\"p\",{children:\"This version of PatrolBot is still in development as of March 2022. This is the full webapp version of the software. The robot was now provided to us by a robotics professor at the University of Nevada, Reno, and the web application controls the frontend and backend. We are using Django with HTML, CSS, and Javascript, on an elastic beanstalk instace. Video data is streamed from a Raspberry Pi on the robot to the Django app where both machine learning models are ran. Commands from the website's frontend are sent to a separate Raspberry Pi on the robot which takes the commands and interprets them as ROS code before sending it to the robot. The repository is currently private.\"})]})]});}export default Projects;","map":{"version":3,"sources":["/Users/brandonbanuelos/Desktop/react_website/src/pages/Projects.js"],"names":["React","useRef","ProjectsList","ProjectItem","PySynthImage","InstaBotImage","I3DDemoImage","PatrolBotV1Image","PatrolBotV2Image","PatrolBotV3Image","Projects","PySynth","InstaBot","PatrolBotV1","PatrolBotV2","I3DDemo","PatrolBotV3","refArray","scroll","ref","current","scrollIntoView","behavior","map","projectItem","key","image","name","year","e","preventDefault","window","location","href"],"mappings":"AAAA,MAAOA,CAAAA,KAAP,EAAeC,MAAf,KAA4B,OAA5B,CACA,OAASC,YAAT,KAA6B,yBAA7B,CACA,MAAO,wBAAP,CACA,MAAOC,CAAAA,WAAP,KAAwB,2BAAxB,CACA,MAAOC,CAAAA,YAAP,KAAyB,uBAAzB,CACA,MAAOC,CAAAA,aAAP,KAA0B,wBAA1B,CACA,MAAOC,CAAAA,YAAP,KAAyB,mBAAzB,CACA,MAAOC,CAAAA,gBAAP,KAA6B,2BAA7B,CACA,MAAOC,CAAAA,gBAAP,KAA6B,2BAA7B,CACA,MAAOC,CAAAA,gBAAP,KAA6B,2BAA7B,C,wFAEA,QAASC,CAAAA,QAAT,EAAoB,CAEhB,GAAMC,CAAAA,OAAO,CAAGV,MAAM,CAAC,IAAD,CAAtB,CACA,GAAMW,CAAAA,QAAQ,CAAGX,MAAM,CAAC,IAAD,CAAvB,CACA,GAAMY,CAAAA,WAAW,CAAGZ,MAAM,CAAC,IAAD,CAA1B,CACA,GAAMa,CAAAA,WAAW,CAAGb,MAAM,CAAC,IAAD,CAA1B,CACA,GAAMc,CAAAA,OAAO,CAAGd,MAAM,CAAC,IAAD,CAAtB,CACA,GAAMe,CAAAA,WAAW,CAAGf,MAAM,CAAC,IAAD,CAA1B,CAEA,GAAMgB,CAAAA,QAAQ,CAAG,CAACN,OAAD,CAAUC,QAAV,CAAoBC,WAApB,CAAiCC,WAAjC,CAA8CC,OAA9C,CAAuDC,WAAvD,CAAjB,CAEA,GAAME,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAACC,GAAD,CAAS,CACpB,GAAGA,GAAG,CAACC,OAAP,CAAe,CACXD,GAAG,CAACC,OAAJ,CAAYC,cAAZ,CAA2B,CAACC,QAAQ,CAAE,QAAX,CAA3B,EACH,CACJ,CAJD,CAMA,mBACI,aAAK,SAAS,CAAC,UAAf,wBACI,WAAI,SAAS,CAAC,eAAd,yBADJ,cAEI,YAAK,SAAS,CAAC,cAAf,UAA+BpB,YAAY,CAACqB,GAAb,CAAiB,SAACC,WAAD,CAAcC,GAAd,CAAsB,CAClE,mBAAO,eAAQ,OAAO,CAAE,yBAAMP,CAAAA,MAAM,CAACD,QAAQ,CAACQ,GAAD,CAAT,CAAZ,EAAjB,uBAA8C,KAAE,WAAF,EAErD,KAAK,CAAED,WAAW,CAACE,KAFkC,CAGrD,IAAI,CAAGF,WAAW,CAACG,IAHkC,CAIrD,IAAI,CAAGH,WAAW,CAACI,IAJkC,EAC9CH,GAD8C,CAA9C,EAAP,CAMH,CAP8B,CAA/B,EAFJ,cAWI,aAAK,SAAS,CAAC,cAAf,wBACI,WAAI,GAAG,CAAEd,OAAT,CAAkB,SAAS,CAAC,SAA5B,qBADJ,cAEQ,YAAK,GAAG,CAAGP,YAAX,EAFR,cAGQ,gfAHR,cAYQ,aAAK,SAAS,CAAC,iBAAf,wBACI,eAAQ,IAAI,CAAC,QAAb,CACA,OAAO,CAAI,iBAACyB,CAAD,CAAO,CAAEA,CAAC,CAACC,cAAF,GACpBC,MAAM,CAACC,QAAP,CAAgBC,IAAhB,CAAqB,8BAArB,CAAqD,CAFrD,CAGA,SAAS,CAAC,cAHV,wBADJ,cAKI,eACA,OAAO,CAAI,iBAACJ,CAAD,CAAO,CAAEA,CAAC,CAACC,cAAF,GAChBC,MAAM,CAACC,QAAP,CAAgBC,IAAhB,CAAqB,4CAArB,CAAmE,CAFvE,CAGA,SAAS,CAAC,aAHV,uBALJ,GAZR,cAsBI,WAAI,GAAG,CAAErB,QAAT,CAAmB,SAAS,CAAC,UAA7B,sBAtBJ,cAuBQ,YAAK,GAAG,CAAGP,aAAX,EAvBR,cAwBQ,oaAxBR,cA+BQ,eACA,OAAO,CAAI,iBAACwB,CAAD,CAAO,CAAEA,CAAC,CAACC,cAAF,GAChBC,MAAM,CAACC,QAAP,CAAgBC,IAAhB,CAAqB,6CAArB,CAAoE,CAFxE,CAGA,SAAS,CAAC,cAHV,uBA/BR,cAmCI,WAAI,GAAG,CAAEpB,WAAT,CAAsB,SAAS,CAAC,aAAhC,yBAnCJ,cAoCQ,YAAK,GAAG,CAAGN,gBAAX,EApCR,cAqCQ,85BArCR,cAqDI,WAAI,GAAG,CAAEO,WAAT,CAAsB,SAAS,CAAC,aAAhC,yBArDJ,cAsDQ,YAAK,GAAG,CAAGN,gBAAX,EAtDR,cAuDQ,g5BAvDR,cAqEQ,eACA,OAAO,CAAI,iBAACqB,CAAD,CAAO,CAAEA,CAAC,CAACC,cAAF,GAChBC,MAAM,CAACC,QAAP,CAAgBC,IAAhB,CAAqB,+CAArB,CAAsE,CAF1E,CAGA,SAAS,CAAC,eAHV,uBArER,cAyEI,WAAI,GAAG,CAAIlB,OAAX,CAAoB,SAAS,CAAC,SAA9B,sBAzEJ,cA0EQ,YAAK,GAAG,CAAGT,YAAX,EA1ER,cA2EQ,83BA3ER,cAyFQ,eACA,OAAO,CAAI,iBAACuB,CAAD,CAAO,CAAEA,CAAC,CAACC,cAAF,GAChBC,MAAM,CAACC,QAAP,CAAgBC,IAAhB,CAAqB,4CAArB,CAAmE,CAFvE,CAGA,SAAS,CAAC,SAHV,uBAzFR,cA6FI,WAAI,GAAG,CAAIjB,WAAX,CAAwB,SAAS,CAAC,aAAlC,yBA7FJ,cA8FQ,YAAK,GAAG,CAAGP,gBAAX,EA9FR,cA+FQ,isBA/FR,GAXJ,GADJ,CAyHH,CAED,cAAeC,CAAAA,QAAf","sourcesContent":["import React, {useRef} from 'react';\nimport { ProjectsList } from '../helpers/ProjectsList';\nimport '../styles/Projects.css';\nimport ProjectItem from '../components/ProjectItem';\nimport PySynthImage from \"../assets/pysynth.png\";\nimport InstaBotImage from \"../assets/instabot.svg\";\nimport I3DDemoImage from \"../assets/i3d.png\";\nimport PatrolBotV1Image from \"../assets/patrolbotv1.png\";\nimport PatrolBotV2Image from \"../assets/patrolbotv2.png\";\nimport PatrolBotV3Image from \"../assets/patrolbotv3.png\";\n\nfunction Projects() {\n\n    const PySynth = useRef(null);  \n    const InstaBot = useRef(null);\n    const PatrolBotV1 = useRef(null);\n    const PatrolBotV2 = useRef(null);\n    const I3DDemo = useRef(null);\n    const PatrolBotV3 = useRef(null);\n\n    const refArray = [PySynth, InstaBot, PatrolBotV1, PatrolBotV2, I3DDemo, PatrolBotV3]\n\n    const scroll = (ref) => {\n        if(ref.current){\n            ref.current.scrollIntoView({behavior: \"smooth\"})\n        }\n    } \n\n    return (\n        <div className=\"projects\">\n            <h1 className=\"projectsTitle\">My Projects</h1>\n            <div className=\"projectsList\">{ProjectsList.map((projectItem, key) => {\n                return <button onClick={() => scroll(refArray[key])}>< ProjectItem \n                key = {key}\n                image={projectItem.image} \n                name ={projectItem.name} \n                year ={projectItem.year}/></button>\n                \n            })}\n            </div>\n            <div className=\"projectsText\">\n                <h1 ref={PySynth} className=\"PySynth\">PySynth</h1>\n                    <img src ={PySynthImage}></img>\n                    <p>This project was term project for my first semester at\n                        university in 2019. It acted as a virtual analog\n                        synthesizer. I programmed this in Python using PyAudio,\n                        Tkinter, NumPy, and MatPlotLib. This project uses a crude\n                        form of UI, but it was my first real project using code. It\n                        was based on additive synthesis and had two oscillators that\n                        generated different types of sound waves based on NumPy functions.\n                        Click below to find my source code and my video presentation of it!\n                    </p>\n                    <div className=\"buttonContainer\">\n                        <button type=\"button\" \n                        onClick = {(e) => { e.preventDefault();\n                        window.location.href=\"https://youtu.be/OitXpHwO8Ho\";}}\n                        className=\"PySynthVideo\">View Video</button>\n                        <button \n                        onClick = {(e) => { e.preventDefault();\n                            window.location.href=\"https://github.com/brandonbanuelos/PySynth\";}}\n                        className=\"PySynthCode\">View Code</button>\n                     </div>\n                <h1 ref={InstaBot} className=\"InstaBot\">InstaBot</h1>\n                    <img src ={InstaBotImage}></img>\n                    <p>This project was something I did using Python and Selenium.\n                        It uses Selenium to access the HTML elements inside of Instagram's\n                        website, so it automates the browsing experience. Instagram is\n                        full of bots these days, so I wondered how difficult it would be \n                        for me to make one. This specific one logs in, goes to a specific hashtag,\n                        and likes and/or comments on posts depending on your user settings.\n                    </p>\n                    <button \n                    onClick = {(e) => { e.preventDefault();\n                        window.location.href=\"https://github.com/brandonbanuelos/InstaBot\";}}\n                    className=\"InstaBotCode\">View Code</button>\n                <h1 ref={PatrolBotV1} className=\"PatrolBotV1\">PatrolBotV1</h1>\n                    <img src ={PatrolBotV1Image}></img>\n                    <p>This project marked the beginning of my senior project with a team of\n                        four other students. This\n                        is an ambitious attempt to combine, software, robotics, and \n                        machine learning, to detect bike theft. This version was dubbed V1 after the fact\n                        because it was a prototype. It used primitive graphics and ran \n                        locally as a desktop software with only one machine learning \n                        model. It was programmed in Python using PyTorch, PyQt5, and\n                        NumPy. It was meant to be ran locally on a Raspberry Pi \n                        which controlled the robot, and the Pi\n                        would be VNC'd in order to view the feed. The machine learning model\n                        was trained on images of bikes, humans, bolt cutters, and angle grinders.\n                        If objects were detected, they would be placed into the logs, so\n                        the user could view any potentially dangerous threats. The repository for this one\n                        has been transformed into the the 1.5 version, so I will include the link\n                        in the next section.\n                    </p>\n                <h1 ref={PatrolBotV2} className=\"PatrolBotV2\">PatrolBotV2</h1>\n                    <img src ={PatrolBotV2Image}></img>\n                    <p>This version of the project marked the next step into the final product.\n                        This version was still ran locally, but it added a state-of-the-art\n                        Slow Fast Model trained on Kinetics 400 to detect actions through\n                        live video. This meant the robot could now detect aggressive actions\n                        coupled with the use of object detection to get a fuller picture of \n                        possible bike theft. This also added the use of IOU (intersection \n                        over union) of objects such as bolt cutters or angle grinders on\n                        top of bikes in order to provide a type of threat model algorithm\n                        Additionally, this version marked an increase in the UI components of \n                        the software. This time the UI was customized using CSS. The link below\n                        contains the 1.5 version of PatrolBot. This means it has the action detection\n                        model and algorithm, but it doesn't have the upgraded UI. This is because\n                        version 2 was still part of the prototype development.\n                    </p>\n                    <button \n                    onClick = {(e) => { e.preventDefault();\n                        window.location.href=\"https://github.com/banuelos-brandon/PatrolBot\";}}\n                    className=\"PatrolBotV1.5\">View Code</button>\n                <h1 ref = {I3DDemo} className=\"I3DDemo\">I3D Demo</h1>\n                    <img src ={I3DDemoImage}></img>\n                    <p>This demo was created to show of my novel solution to a problem\n                        my group encountered during the development of PatrolBotV3. The issue\n                        was that the action detection model ran on mp4 files rather than lists\n                        or arrays of frames. PatrolBotV3 was the transformation from a locally-ran\n                        application to a web application, so saving and loading mp4 files would have\n                        added too much complication. Instead I decided to find a way to save a series\n                        of frames from the webcam and format them into a way that was readable by the \n                        Slow Fast or Inflated 3D models for action detection. This demo also\n                        shows off the training on this model done by my team. We trained this model\n                        to detect if behavior is aggressive or not. This demo takes\n                        webcam input and stores it during the duration of running time \n                        in the code, and predicts whether or not behavior from the video clip \n                        is agressive or not.\n                    </p>\n                    <button\n                    onClick = {(e) => { e.preventDefault();\n                        window.location.href=\"https://github.com/brandonbanuelos/I3DDemo\";}} \n                    className=\"I3DCode\">View Code</button>\n                <h1 ref = {PatrolBotV3} className=\"PatrolBotV3\">PatrolBotV3</h1>\n                    <img src ={PatrolBotV3Image}></img>\n                    <p>This version of PatrolBot is still in development as of March 2022. \n                        This is the full webapp version of the software. The robot was now \n                        provided to us by a robotics professor at the University of Nevada, Reno, and \n                        the web application controls the frontend and backend. We are using Django with\n                        HTML, CSS, and Javascript, on an elastic beanstalk instace. Video data\n                        is streamed from a Raspberry Pi on the robot to the Django app where both machine\n                        learning models are ran. Commands from the website's frontend are sent to\n                        a separate Raspberry Pi on the robot which takes the commands and interprets them\n                        as ROS code before sending it to the robot. The repository is currently private.\n                    </p>\n                </div>\n        </div>\n\n    );\n}\n\nexport default Projects;"]},"metadata":{},"sourceType":"module"}